---
title: "Geostatistics Athens Week project"
author: "SD Team"
date: "November 17, 2021"
output: 
    pdf_document:
        number_sections: true
        toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The Jura data set
The Jura data set comprises seven heavy metals measured in the top soil of the swiss Jura, along with consistently coded land use and rock type factors, as well as geographic coordinates. 

Variable description :

* Xloc: X coordinate, local grid km
* Yloc: Y coordinate, local grid km
* Landuse: Land use: 1: Forest, 2: Pasture (Weide(land), Wiese, Grasland), 3: Meadow (Wiese, Flur, Matte, Anger), 4: Tillage (Ackerland, bestelltes Land)
* Rock: Rock Types: 1: Argovian, 2: Kimmeridgian, 3: Sequanian, 4: Portlandian, 5: Quaternary.
* Cd: mg cadmium kg^-1 topsoil
* Co: mg cobalt kg^-1 topsoil
* Cr: mg chromium kg^-1 topsoil
* Cu: mg copper kg^-1 topsoil
* Ni: mg nickel kg^-1 topsoil
* Pb: mg lead kg^-1 topsoil
* Zn: mg zinc kg^-1 topsoil

You are given three different files:

* jura_pred.csv: learning dataset
* jura_grid.csv: prediction grid (contains locations and covariables)
* jura_val_loc: validation locations and covariables

# Exploratory analysis

```{r}
rm(list=ls()) #Clean the working directory
```
## Basic statistics
1. load the dataset from jura_pred.csv (on the cloud)
```{r}
#Data set
jura = read.csv("../../jura/jura_pred.csv")
```
2. What is the class of the dataset?
```{r}
class(jura)
```
3. What is the number of observations? What is the number of variables?
```{r}
str(jura) #e.g.
```
4. Print the name of the variables.
```{r}
names(jura)
```
5. Compute the minimum and maximum value for each coordinate.
```{r}
min(jura$Xloc);min(jura$Yloc)
apply(jura[,1:2],2,min)
range(jura$Xloc)
max(jura$Xloc);max(jura$Yloc)
apply(jura[,1:2],2,max)
range(jura$Yloc)
```
6. Compute basic statistics for the seven different heavy metals (mean, min, max, quartiles and standard deviation)
```{r}
summary(jura[,5:11])
apply(jura[,5:11],2,sd)
```
7. Compute the mean of cobalt concentration for the four different landuses 
```{r}
mean(jura[jura$Landuse==1,6],na.rm=T)
mean(jura[jura$Landuse==2,6],na.rm=T)
mean(jura[jura$Landuse==3,6],na.rm=T)
mean(jura[jura$Landuse==4,6],na.rm=T)
```

### Graphical Representations
1. Apply the plot function to the whole dataset
```{r}
plot(jura)
```
2. Plot the coordinates.
```{r}
plot(jura[,1:2])
```
3. On the same plot, display the points with landuse 2 (pasture) in red.
```{r}
plot(jura[,1:2])
points(jura[jura$Landuse==2,1:2],col='red')
```
4. Plot the seven, heavy metal concentrations histograms.
```{r}
par(mfrow=c(2,4)) # on a grid of 2x4 plots
hist(jura$Cd)
hist(jura$Co)
hist(jura$Cr)
hist(jura$Cu)
hist(jura$Ni)
hist(jura$Pb)
hist(jura$Zn)
```
5. Plot the seven heavy metal concentrations as functions of the landuse
```{r}
par(mfrow=c(2,4)) 
plot(jura$Landuse,jura$Cd)
plot(jura$Landuse,jura$Co)
plot(jura$Landuse,jura$Cr)
plot(jura$Landuse,jura$Cu)
plot(jura$Landuse,jura$Ni)
plot(jura$Landuse,jura$Pb)
plot(jura$Landuse,jura$Zn)
```
6. Plot the seven heavy metal concentrations as functions of the rocktype
```{r}
par(mfrow=c(2,4)) 
plot(jura$Rock,jura$Cd)
plot(jura$Rock,jura$Co)
plot(jura$Rock,jura$Cr)
plot(jura$Rock,jura$Cu)
plot(jura$Rock,jura$Ni)
plot(jura$Rock,jura$Pb)
plot(jura$Rock,jura$Zn)

```

### Some statistics
1. Transform the variables Landuse and Rock into factors (see ?as.factors).
```{r}
jura$Landuse = as.factor(jura$Landuse)
jura$Rock = as.factor(jura$Rock)
```

2. Use the function aov to compute the analysis of variance of the cobalt concentrations with Landuse, Rock and their product as factors.

```{r}
aov.co = aov(Co~Landuse+Rock+Landuse*Rock,data=jura)
summary(aov.co)
```

3. Do the same on the other concentrations (check the histograms prior to apply a transformation if necessary).
```{r}
aov.cd = aov(Cd~Landuse+Rock+Landuse*Rock,data=jura)
summary(aov.cd)

aov.cr = aov(Cr~Landuse+Rock+Landuse*Rock,data=jura)
summary(aov.cr)

aov.cu = aov(Cu~Landuse+Rock+Landuse*Rock,data=jura)
summary(aov.cu)

aov.ni = aov(Ni~Landuse+Rock+Landuse*Rock,data=jura)
summary(aov.ni)

aov.pb = aov(Pb~Landuse+Rock+Landuse*Rock,data=jura)
summary(aov.pb)

aov.zn = aov(Zn~Landuse+Rock+Landuse*Rock,data=jura)
summary(aov.zn)
```

# Utilities
Load RGeostats.

```{r}
library(Rcpp) 
library(RGeostats) 
constant.define("asp",1) #environment variable for the scale ratio between axes
```

Load the data set and the prediction grid

```{r}
#Data set
jura_tot = read.csv("../../jura/jura_pred.csv")
#Prediction grid
grid = read.csv("../../jura/jura_grid.csv")
names(jura_tot)
names(grid)
```

Coding of the factors
```{r}
jura_tot$Landuse = as.factor(jura_tot$Landuse)
jura_tot$Rock = as.factor(jura_tot$Rock)

grid$Landuse = as.factor(grid$Landuse)
grid$Rock = as.factor(grid$Rock)
```

Change the names of the modalities to be consistent with their names on the grid

```{r}
levels(jura_tot$Landuse)
levels(grid$Landuse)
levels(jura_tot$Landuse)=c("Forest","Pasture","Meadow","Tillage")

levels(jura_tot$Rock)
levels(grid$Rock)
levels(jura_tot$Rock)=c("Argovian","Kimmeridgian","Sequanian","Portlandian","Quaternary")
```

Separate the data set in two sets : the training set and the validation set.
For the project, you should use the full data set for the training.
You submit the prediction on Kaggle for a set of locations on which you only know the locations and the factors.

```{r}
set.seed(1234) #Set the seed of the random generators

ntot = nrow(jura_tot)
ntrain = 200
nval = ntot - ntrain

indtrain = sample(ntot,ntrain)
indval = setdiff(1:ntot,indtrain)

jura = jura_tot[indtrain,]
val_loc = jura_tot[indval,1:4]

#val contains the values to predict. For the project, these values will be on Kaggle
#(for other locations) and you won't know them
#You will have the locations and covariables at the unknown locations by the following command :

#val_loc = read.csv("jura/jura_val_loc.csv")

val = cbind(1:nval,jura_tot[indval,]$Co)
```

RGeostats target grid and utilities for display 

```{r}
gridtemp = db.create(grid)
gridtemp = db.locate(gridtemp,c("Xloc","Yloc"),"x")
nx = c(length(unique(grid[,1])),length(unique(grid[,2])))
gridrg = db.grid.init(gridtemp,nodes=nx)
gridrg = migrate(gridtemp,gridrg,names=4:gridtemp$natt,radix="")
gridrg = db.rename(gridrg,2:3,c("Xloc","Yloc"))
gridrg = db.sel(gridrg,!is.na(gridrg[,"Landuse"])&!is.na(gridrg[,"Rock"]))

add.variable =function(var,grid,varname,gridtemp.=gridtemp)
{
  tt=db.add(gridtemp.,var)
  tt=db.rename(tt,tt$natt,varname)
  grid = migrate(tt,grid,names=tt$natt,radix="")
  grid
}
```


# Interpolation exercise

1. Prediction by the mean

In this method we will predict the concentration of Cobalt using the mean of the training dataset.


```{r}
mean((val[,2]-mean(val[,2]))^2)
```

Mean Squared Error = 12.617

Interpretation of this method: Using this method is impractical because it predicts the same concentration of Co in all tests without taking into account other factors that may help find the true concentration of Co.


2. ANOVA 

Analysis of Variance (ANOVA) is a statistical technique, commonly used to studying differences between two or more group means. ANOVA test is centred on the different sources of variation in a typical variable. ANOVA in R primarily provides evidence of the existence of the mean equality between the groups. This statistical method is an extension of the t-test. It is used in a situation where the factor variable has more than one group.


```{r}
library("ggplot2")
ggplot(jura, aes(x = Landuse, y = Co, fill = Landuse)) +
    geom_boxplot() +
    geom_jitter(shape = 15,
        color = "steelblue",
        position = position_jitter(0.21)) +
    theme_classic()
```
```{r}
ggplot(jura, aes(x = Rock, y = Co, fill = Rock)) +
    geom_boxplot() +
    geom_jitter(shape = 15,
        color = "steelblue",
        position = position_jitter(0.21)) +
    theme_classic()
```

H0: The means are equal for both variables
H1: The means are different for both variables

```{r,eval=F}
#Fit the anova model
aov.co = aov(Co~Landuse+Rock,data=jura)
summary(aov.co)
```

We can conclude that both Landuse and Rock are statistically different from 0. You can reject the NULL hypothesis and confirm that changing the Landuse or Rock impact the Co concentration.


```{r}

#Prediction on the validation locations
res.aov.val=predict(aov.co,val_loc)

##
#Compute score 
mean((res.aov.val-val[,2])^2)


#Prediction on the grid
res.aov=predict(aov.co,grid)

##
#Add to the RGeostats db and display
gridrg = add.variable(res.aov,gridrg,"aov.predict")
plot(gridrg,pos.legend=1)
```

Mean Squared Error = 9.7


3. Linear regression of functions of coordinates

Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data.



```{r}
#Fit the linear model

trend = lm(Co~Xloc+Yloc+Xloc*Yloc+I(Xloc^2)+I(Yloc^2),data=jura)
summary(trend)

#Prediction on the validation locations
res.trend.val=predict(trend,val_loc)

##
#Compute score 
mean((res.trend.val-val[,2])^2)
##

#Prediction on the grid
res.trend=predict(trend,grid)

#Add to the RGeostats db and display
gridrg = add.variable(res.trend,gridrg,"trend.predict")
plot(gridrg,pos.legend=1)
```

Mean Squared Error = 11.57367



4.  N-Nearest neighbours


```{r}
ns = 3 #number of samples to consider for the prediction
datrg = db.create(jura)
datrg = db.locate(datrg,2:3,"x")
datrg = db.locate(datrg,7,"z")
m = model.create(1)
neigh=neigh.create(radius=100,nmini=ns,nmaxi=ns)


###
valrg=db.create(val_loc)
valrg=db.locate(valrg,2:3,"x")
res.val = kriging(datrg,valrg,m,neigh)
mean((res.val[,6]-val[,2])^2,na.rm=T)
###


res = kriging(datrg,gridrg,m,neigh)
plot(res)

```

Mean Squared Error = 7.08



5. Inverse distance
```{r}
degree = 4
datrg = db.create(jura)
datrg = db.locate(datrg,2:3,"x")
datrg = db.locate(datrg,7,"z")


###
valrg=db.create(val_loc)
valrg=db.locate(valrg,2:3,"x")
res.val = invdist(datrg,valrg,exponent = degree)
mean((res.val[,6]-val[,2])^2)

res = invdist(datrg,gridrg)
plot(res)

```

Mean Squared Error = 7.09



6. Improve the prediction by using other parameterizations for the previous methods.

Taking number of samples = 4 in N-Nearest neighbours

```{r}
ns = 4 #number of samples to consider for the prediction
datrg = db.create(jura)
datrg = db.locate(datrg,2:3,"x")
datrg = db.locate(datrg,7,"z")
m = model.create(1)
neigh=neigh.create(radius=100,nmini=ns,nmaxi=ns)


###
valrg=db.create(val_loc)
valrg=db.locate(valrg,2:3,"x")
res.val = kriging(datrg,valrg,m,neigh)
mean((res.val[,6]-val[,2])^2,na.rm=T)
###


res = kriging(datrg,gridrg,m,neigh)
plot(res)

```
Mean Squared Error : 6.58

7. Use other methods (Local Polynomial Regression, Random Forests, ...).


Random Forest
Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time.

```{r}

library(randomForest)
rf <- randomForest(formula = Co ~ Landuse+Rock, mtry = 4,
                         nodesize = 5,
                         ntree=500,
                         data = jura)

rf.val=predict(rf,val_loc)

##
#Compute score 
mean((rf.val-val[,2])^2)
##

#Prediction on the grid
res.rf=predict(rf,grid)

#Add to the RGeostats db and display
gridrg = add.variable(res.rf,gridrg,"rf.predict")
plot(gridrg,pos.legend=1)
```
Mean Squared Error : 8.16


Local polynomial regression

```{r}

local_pol_reg=loess(formula= Co ~ Xloc+Yloc, data=jura)
ggplot(jura, aes(Xloc+Yloc, Co) ) +
  geom_point() +
  stat_smooth(method = loess, formula = y ~ x)

#Prediction on the validation locations
local_pol_reg.val=predict(local_pol_reg,val_loc)
local_pol_reg.val[is.na(local_pol_reg.val)] <- mean(local_pol_reg.val, na.rm = TRUE)

##
#Compute score 
mean((local_pol_reg.val-val[,2])^2)
##

#Prediction on the grid
res.local_pol_reg=predict(local_pol_reg,grid)

#Add to the RGeostats db and display
gridrg = add.variable(res.local_pol_reg,gridrg,"trend.predict")
plot(gridrg,pos.legend=1)
```
Mean Squared Error : 8.6139


# Univariate analysis

## Variography

### Experimental variogram (isotropic case)

1. Compute and plot the experimental variogram of the cobalt concentration (using `vario.calc()`). Try different values of lag and comment the results.

For starters it is important for us to outlie the importance of the variogram when dealing with structural analysis, we will also reinforce the fact that we are dealing with second order stationary random functions.

For a random function Z(x) the variogram is calculated as :

$\gamma(h) =  \frac{1}{2} . Var[Z(x+h) - Z(x)]$

Hence the variogram shows how the difference between the random functions Z(x+h) and Z(x) evolve as a function of the lag h.

The variogram is a powerful tool because unlike the covariance , it does not require a knowledge of the mean which has to be estimated in the case of the covariance and hence could introduce a bias.

We will now proceed to some simulations on R studio , mainly plotting the variogram of the cobalt concentrations with different values of lags.

We will limit our study on lags up till 3. We will start our simulation with 10 lag points , that is a 0.3 lag between a random function and its translated version.

Create the db :

```{r}
library(RGeostats)
jurarg = db.create(jura)
jurarg = db.locate(jurarg,c("Xloc","Yloc"),"x")
jurarg = db.locate(jurarg,"Co","z")
```

Compute the empirical variogram (with the function *vario.calc*)

```{r}
v = vario.calc(jurarg,nlag=10)
```

Fit a model (with *model.auto*)

```{r}
m = model.auto(v,struct = c(1,8))
```

It is intuitive to think that points close to each other (having a low lag) should have similar properties and hence assume close values , and this is further proved by the results of the simulation , when the lag is low (0.3 for the first point for example) the dissimilarity between the two points is low , the dissimilarities further increase when the lag increases. The solid smooth line is the fitted model generated using model.auto.

Let us now increase the value of the lag points to 100 instead of 10 , this will reduce the lag increment by 10 , we get the following results:

```{r}
v = vario.calc(jurarg,nlag=100)
m = model.auto(v,struct = c(1,8))
```

Similarly we can also see at lower lag values the variogram produces smaller results indicated that close points have similar properties.

Let us now increase the value of the lag point to 1000 , we get the following results :

```{r}
v = vario.calc(jurarg,nlag=1000)
m = model.auto(v,struct = c(1,8))
```

At lower lag levels the variogram produces very small values , at higher lag values the variogram produces higher values , however one can notice that beyond a certain point the variogram values do not change much that is Z(x+h) and Z(x) become uncorrelated from that point onward , we are lead to think that this variogram has a sill.


2. Print the number of pairs of points used to compute the variogram values (using the options `npairpt=TRUE,npairdw=TRUE` in `plot()`) for different values of lag and comment the results.

Now let us print the number of pairs of points used to represent these variograms , for a lag value of 10,50,100 respectively we get the following results :

```{r}
v = vario.calc(jurarg,nlag=10)
draw.vario(v,npairpt = T,npairdw = T)
```

```{r}
v = vario.calc(jurarg,nlag=50)
draw.vario(v,npairpt = T,npairdw = T)
```

```{r}
v = vario.calc(jurarg,nlag=100)
draw.vario(v,npairpt = T,npairdw = T)
```

As we can see from the simulation results when the lag h decreases ( or respectively  the number of lag points increase) the number of pairs decrease considerably , and this is obvious because when the lag h decreases there will be less and less points within that lag distance and hence the number of pairs of points within that lag h of each other should also decrease.

### Experimental variogram (anisotropic case)

1. Compute and plot the variogram maps (using `vmap.calc()`) the Cobalt concentration order to check for anisotropies. Comment

So far we have only considered the variogram in the isotropic case in which the variogram does not vary with direction , it is only a function of the modulus of the lag h.
We will now move on to the anisotropic case where not only the modulus of h is of importance but the direction of h is also important , to check for these anisotropies we will need to plot the variogram map of the cobalt concentration : 

Compute a variogram map
```{r}
plot(vmap.calc(jurarg))
```

We can obviously see that the behavior of the lag in different directions is not the same as the colors in the variogram map tend to change with direction , for example in a -45 degrees direction the dissimilarities between Z(x+h) and Z(x) is way higher than in a 0 degrees direction and this will be shown in the upcoming variograms.

2. Compute and plot directional variograms (according to the anisotropy directions determined with the maps).

We will now plot the variograms for different directions namely 0 , 45 and -45 degrees:


Directional variograms
```{r}
vdir = vario.calc(jurarg,nlag=10,dir=c(0,-45,45))
plot(vdir)
```

The black curve computes the variogram in the 0 degrees direction, the red curve computes it in the -45 degrees direction and the green curve in the 45 degrees direction , the behavior for these 3 curves is quite similar when the lag is small , however when the lag increases we can see that the difference between Z(x+h) and Z(x) is way higher for the red curve (+45 degrees direction) however this is not enough to say the data is anisotropic.

If we try to fit a model for these curves we obtain:

Fit a model

```{r}
maniso = model.auto(vdir,struct=c(1,2,3,4))
```

The model fit suggests that the sill is constant however the range for which the sill is obtained is different for the three curves and hence varies with direction suggesting that there may be geometric anisotropies but no zonal anisotropies.




### Model adjustement

The function `melem.name()` gives the models available in RGeostats.

1. Adjust a model using the function `model.auto()` (isotropic and anisotropic cases) on experimental variograms and print the model caracteristics.
2. Try imposing different structures or combinations of structures.

Let us first try different models for the isotropic case

K-Bessel function with nugget effect
```{r}
model.auto(v,struct=c(1,8))
```

Gaussian function with nugget effect
```{r}
model.auto(v,struct=c(1,4))
```

Exponential function with nugget effect
```{r}
model.auto(v,struct=c(1,2))
```

Exponential,spherical,K-Bessel function with nugget effect
```{r}
model.auto(v,struct=c(1,2,3,8))
```

Let us try the same models for the anisotropic case

K-Bessel function with nugget effect
```{r}
model.auto(vdir,struct=c(1,8))
```

Gaussian function with nugget effect
```{r}
model.auto(vdir,struct=c(1,4))
```

Exponential function with nugget effect
```{r,eval=FALSE}
model.auto(vdir,struct=c(1,2))
```

Exponential,spherical,K-Bessel function with nugget effect
```{r}
model.auto(vdir,struct=c(1,2,3,8))
```

3. Compare the models adjusted on the experimental variogram and the variogram map (using `vmap.auto()`).

```{r}
vmap.auto(vmap.calc(jurarg),struct=c(1,8))
```

```{r}
vmap.auto(vmap.calc(jurarg),struct=c(1,4))
```

```{r}
vmap.auto(vmap.calc(jurarg),struct=c(1,2))
```

```{r}
vmap.auto(vmap.calc(jurarg),struct=c(1,2,3,8))
```

## Prediction

### Ordinary Kriging

1. Compute and plot the ordinary kriging of the cobalt over the prediction grid. Plot the associated standard deviation map.
2. Try several variogram models (basic structures, anisotropy), and neighborood options. Compute the prediction scores. Comment the results


We will now move on to estimating the cobalt concentrations of the test results using the ordinary kriging method , let us state first that the ordinary kriging method is an interpolation method that assumes that the mean is unknown but constant, it uses the variogram as the basis.

We first have to create the target grid using the following code :

```{r}
jurarg = db.create(jura)
jurarg = db.locate(jurarg,c("Xloc","Yloc"),"x")
jurarg = db.locate(jurarg,"Co","z")

v = vario.calc(jurarg,nlag=500)

m = model.auto(v,struct = c(1,8))

grid = read.csv("../../jura/jura_grid.csv")
gridtemp = db.create(grid)
gridtemp = db.locate(gridtemp,c("Xloc","Yloc"),"x")
nx = c(length(unique(grid[,1])),length(unique(grid[,2])))
gridrg = db.grid.init(gridtemp,nodes=nx)
gridrg = migrate(gridtemp,gridrg,names=4:gridtemp$natt,radix="")
gridrg = db.rename(gridrg,2:3,c("Xloc","Yloc"))
gridrg = db.sel(gridrg,!is.na(gridrg[,"Landuse"])&!is.na(gridrg[,"Rock"]))
```

Next we will have to apply the ordinary kriging on the grid we have just created and plot the result:

```{r}
neigh = neigh.create(type = 0)
res=kriging(jurarg,gridrg,m,neigh)
plot(res)
```

This plot shows us the different possible estimated cobalt concentrations based on the Xloc on the horizontal axis and the Yloc on the vertical axis , the red color indicating high cobalt concentration , the blue color indicating lower cobalt concentration while the green and yellow colors are inbetween.

Next we can apply the ordinary kriging on the validation set using the following code:

```{r}
neigh = neigh.create(nmini=10,nmaxi=30,radius=1)
val_locrg = db.create(val_loc)
val_locrg = db.locate(val_locrg,c("Xloc","Yloc"),"x")
res_val=kriging(jurarg,val_locrg,m,neigh)
mean((res_val[,"Kriging*estim"]-val[,2])^2)
```

The neighborhood we took into consideration here is a moving neighborhood with 10 minimum points in the neighborhood and 30 maximum points in the neighborhood.

We get a prediction score of : 7.176295

As an attempt to improve this prediction score we will try to use different combinations of basic structures, let us first try using the sector option in the neighborhood:

```{r}
neigh = neigh.create(nmini=10,nmaxi=30,radius=3,flag.sector = T,nsect=12,nsmax=30)
val_locrg = db.create(val_loc)
val_locrg = db.locate(val_locrg,c("Xloc","Yloc"),"x")
res_val=kriging(jurarg,val_locrg,m,neigh)
mean((res_val[,"Kriging*estim"]-val[,2])^2)
```

This gives us a prediction score of  : 7.146272 which is an improvement of the previous score.

Let us attempt an anisotropic model as opposed to the previous isotropic model , for this purpose the radius can no longer be constant but has to be a vector equal to the dimension we are working with. In this case the dimension is equal to 2 since we are only working alongside 2 parameters the Xloc and Yloc. We will try an ellipsoid c(3,3.5) and rotating this ellipsoid a -45 degrees angle , the code is as follows : 

Ordinary kriging on the validation set anisotropic case

```{r}
v = vario.calc(jurarg,nlag=100)
m = model.auto(v,struct = c(1,2))
radvec1=c(3,3.5)
angle=-45
rotmat1 = matrix(
  c(cos(angle), -sin(angle), sin(angle), cos(angle)),
  nrow = 2,            
  ncol = 2,            
  byrow = TRUE         
)
neigh=neigh.create(type=2,nmini=10,nmaxi=30,radius=radvec1,flag.sector=T,flag.aniso=T,nsect=10,nsmax=30,flag.rotation=T,rotmat = rotmat1)
neigh = neigh.create(nmini=10,nmaxi=30,radius=1)
val_locrg = db.create(val_loc)
val_locrg = db.locate(val_locrg,c("Xloc","Yloc"),"x")
res_val=kriging(jurarg,val_locrg,m,neigh)
mean((res_val[,"Kriging*estim"]-val[,2])^2)
```

The prediction score we got was :  7.942153 which is worse than the isotropic case.

We will try right now changing the structure , the structure we have used so far c(1,8) takes into account the nugget effect and generates a K-bessel model , we will now attempt a power model incorporating the nugget effect c(1,13) : 

```{r}
v = vario.calc(jurarg,nlag=300)
m = model.auto(v,struct = c(1,13))
neigh = neigh.create(nmini=10,nmaxi=30,radius=2.5)
val_locrg = db.create(val_loc)
val_locrg = db.locate(val_locrg,c("Xloc","Yloc"),"x")
res_val=kriging(jurarg,val_locrg,m,neigh)
mean((res_val[,"Kriging*estim"]-val[,2])^2)
```


The prediction score we get is : 7.011163 which is an improvement to the previous score.

Let us now try the order 1 G-C model , using :

```{r}
v = vario.calc(jurarg,nlag=300)
m = model.auto(v,struct = c(1,14))
neigh = neigh.create(nmini=10,nmaxi=30,radius=2.5)
val_locrg = db.create(val_loc)
val_locrg = db.locate(val_locrg,c("Xloc","Yloc"),"x")
res_val=kriging(jurarg,val_locrg,m,neigh)
mean((res_val[,"Kriging*estim"]-val[,2])^2)
```

The prediction score we get is : 6.783702 which is a huge improvement , we could have also done a combination of models such as c(1,8,2,5).

We could also try changing the type of the neighborhood for example : 

```{r}
v = vario.calc(jurarg,nlag=300)
m = model.auto(v,struct = c(1,14))
neigh = neigh.create(type=0)
val_locrg = db.create(val_loc)
val_locrg = db.locate(val_locrg,c("Xloc","Yloc"),"x")
res_val=kriging(jurarg,val_locrg,m,neigh)
mean((res_val[,"Kriging*estim"]-val[,2])^2)
```

However this gives a prediction score of : 6.838635 which is worse than when we used the moving neighborhood.


.

### Universal kriging

Use the indicators of the different levels of the factors (*Rock* and *Landuse*) as covariates to compute the universal kriging prediction. 

Transformation of the Rock factor into indicators

```{r}
indiccut = limits.create(mini=c(1,2,3),maxi=c(2,3,4))

jurarg_KU =db.indicator(jurarg,indiccut,name="Rock")
jurarg_KU = db.locate(jurarg_KU,"Indicator*","f")
jurarg_KU = db.locate(jurarg_KU,"Co","z")

gridrg_KU=db.indicator(gridrg,indiccut,name="Rock")
gridrg_KU = db.locate(gridrg_KU,"Indicator*","f")
gridrg_KU = db.locate(gridrg_KU,"Co","z")

val_locrg_KU=db.indicator(val_locrg,indiccut,name="Rock")
val_locrg_KU = db.locate(val_locrg_KU,"Indicator*","f")
val_locrg_KU = db.locate(val_locrg_KU,"Co","z")
```

```{r}
jurarg_KU.Rock.1
```

Variogram of the residuals

```{r}
drift = c("1","f1","f2","f3")
vres = vario.calc(jurarg_KU,nlag=10,uc=drift)
plot(v)
plot(vres,add=T,col=2)
mres=model.auto(vres,struct=c(1,2))
```

Universal kriging on the grid

```{r}
neigh=neigh.create(type=0)
res_gridKU=kriging(jurarg_KU,gridrg_KU,mres,neigh,uc=drift)
plot(res_gridKU)
```

Plot the associated standard deviation map.

Universal kriging on the validation set and prediction score

```{r}
res_valKU=kriging(jurarg_KU,val_locrg_KU,mres,neigh,uc=drift)
mean((res_valKU[,"Kriging*estim"]-val[,2])^2)
```

1. Add the *Landuse* predictor to the model. 
2. Try several variogram models (basic structures, anisotropy), and neighborood options. Compare the prediction scores. Comment the results.


The difference between the universal kriging and the ordinary kriging lies in the mean , while the mean is still unknown in the universal kriging  , it is no longer a constant and has a drift model that depends on the coordinates , the provided code used “Rock” as an indicator and gave a prediction score of 7.225692 , let us try adding the Landuse predictor to the model instead : 

Transformation of the factors into indicators

```{r}
indiccut = limits.create(mini=c(1,2,3),maxi=c(2,3,4))

jurarg_KU=db.indicator(jurarg,indiccut,name="Landuse")
jurarg_KU = db.locate(jurarg_KU,"Indicator*","f")
jurarg_KU = db.locate(jurarg_KU,"Co","z")

gridrg_KU=db.indicator(gridrg,indiccut,name="Landuse")
gridrg_KU = db.locate(gridrg_KU,"Indicator*","f")
gridrg_KU = db.locate(gridrg_KU,"Co","z")

val_locrg_KU=db.indicator(val_locrg,indiccut,name="Landuse")
val_locrg_KU = db.locate(val_locrg_KU,"Indicator*","f")
val_locrg_KU = db.locate(val_locrg_KU,"Co","z")
```

Variogram of the residuals

```{r}
drift = c("1","f1","f2","f3")
vres = vario.calc(jurarg_KU,nlag=10,uc=drift)
plot(v)
plot(vres,add=T,col=2)
mres=model.auto(vres,struct=c(1,2))
```

Universal kriging on the validation set

```{r}
drift = c("1","f1","f2","f3")
vres = vario.calc(jurarg_KU,nlag=100,uc=drift)
mres=model.auto(vres,struct=c(1,8))
neigh = neigh.create(type=0)
res_valKU=kriging(jurarg_KU,val_locrg_KU,mres,neigh,uc=drift)
mean((res_valKU[,"Kriging*estim"]-val[,2])^2)
```

Let us try now changing the models , the model that was used c(1,8) takes into account the nugget effect while simulating a K-bessel model ,let us try an exponential model with c(1,2) instead  : 

```{r}
mres=model.auto(vres,struct=c(1,2))
res_valKU=kriging(jurarg_KU,val_locrg_KU,mres,neigh,uc=drift)
mean((res_valKU[,"Kriging*estim"]-val[,2])^2)
```

The prediction score becomes 7.01927 which is considerably worse than when the K-Bessel model was used.

Let us try the “Order-1-Gc” model given by c(1,14) : 
We get a prediction score of 6.10542 which is considerably better than both cases.
```{r}
mres=model.auto(vres,struct=c(1,14))
res_valKU=kriging(jurarg_KU,val_locrg_KU,mres,neigh,uc=drift)
mean((res_valKU[,"Kriging*estim"]-val[,2])^2)
```

So far we had our neighborhood type set to  0 which is the unique neighborhood case Let us now try the case of the moving neighborhood , with sectors and anisotropies , the code becomes :

Transformation of the factors into indicators

```{r}
indiccut = limits.create(mini=c(1,2,3),maxi=c(2,3,4))

jurarg_KU=db.indicator(jurarg,indiccut,name="Landuse")
jurarg_KU = db.locate(jurarg_KU,"Indicator*","f")
jurarg_KU = db.locate(jurarg_KU,"Co","z")

gridrg_KU=db.indicator(gridrg,indiccut,name="Landuse")
gridrg_KU = db.locate(gridrg_KU,"Indicator*","f")
gridrg_KU = db.locate(gridrg_KU,"Co","z")

val_locrg_KU=db.indicator(val_locrg,indiccut,name="Landuse")
val_locrg_KU = db.locate(val_locrg_KU,"Indicator*","f")
val_locrg_KU = db.locate(val_locrg_KU,"Co","z")
```

Variogram of the residuals

```{r}
drift = c("1","f1","f2","f3")
vres = vario.calc(jurarg_KU,nlag=10,uc=drift)
plot(v)
plot(vres,add=T,col=2)
mres=model.auto(vres,struct=c(1,2))
```

Universal kriging on the grid

```{r}
neigh=neigh.create(type=0)
res_gridKU=kriging(jurarg_KU,gridrg_KU,mres,neigh,uc=drift)
plot(res_gridKU)
```

Universal kriging on the validation set

```{r}
drift = c("1","f1","f2","f3")
vres = vario.calc(jurarg_KU,nlag=100,uc=drift)
mres=model.auto(vres,struct=c(1,14))
radvec=c(2.2,2.6)
neigh = neigh.create(type=2,nmini=5,nmaxi=5,radius=radvec,flag.sector = T,flag.aniso=T,nsect = 52,nsmax = 100)
res_valKU=kriging(jurarg_KU,val_locrg_KU,mres,neigh,uc=drift)
mean((res_valKU[,"Kriging*estim"]-val[,2])^2)
```

This prediction score is better than anything we have seen so far. 



*Optional:*

You can do the same with the interaction. Hint: consider the product *Landuse* x *Rock* as a new factor. You will have to group some of the levels so as to have well balanced groups. See the functions *replicates* and *TukeyHSD*.


# Multivariate analysis
## Variography

1. Compute the  empirical directional variograms and covariograms of a carefully chosen (justify) set of variables.
What would you conclude about anisotropy ?

2. Fit a model (with *model.auto*).


Let us compute the empirical directional variograms and covariograms of the variables,  we will use cobalt zinc and copper as the regionalized varaibles just as follows , Xloc and Yloc have been previously set as the space dimension.

```{r}
jurarg = db.locate(jurarg,c("Co","Zn","Cu"),"z")
vdir = vario.calc(jurarg,nlag=10,dir=c(-10,35,80,125))
plot(vdir)
```

The plots on the diagonals are the specific variograms for Cobalt , Zinc and Copper , and the ones excluded from the diagonal are the cross variograms between each of the two metals , each colored curve in each plot represents one of the directions in our dir column vector defined above.

When first looking at the colored curves , the data doesn’t seem to be anisotropic even though the values don’t tend to be the same when the lag increases , this difference isn’t that significant to say that the sill is different or the data is zonal anisotropic. The range also doesn’t seem to vary with direction which leads us to think that the data isn’t geometric anisotropic.

We will now fit a model to these directional variograms:

```{r}
m = model.auto(vdir,struct=c(1,8))
```

The model that was fitted coincides for all the directional curves which reinforces our statement above.


## Prediction

1. Interpolate *Co* on the grid using Ordinary Cokriging (function *kriging*) and plot the resulting map as well as the standard deviation map.
Compute the prediction score.

2. You can also try to implement the universal cokriging (*optional*).


Let us now try to interpolate the Co concentrations using the ordinary cokriging , the code is as follows : 

Creation of the target grid.

```{r}
grid = read.csv("../../jura/jura_grid.csv")
gridtemp = db.create(grid)
gridtemp = db.locate(gridtemp,c("Xloc","Yloc"),"x")
nx = c(length(unique(grid[,1])),length(unique(grid[,2])))
gridrg = db.grid.init(gridtemp,nodes=nx)
gridrg = migrate(gridtemp,gridrg,names=4:gridtemp$natt,radix="")
gridrg = db.rename(gridrg,2:3,c("Xloc","Yloc"))
gridrg = db.sel(gridrg,!is.na(gridrg[,"Landuse"])&!is.na(gridrg[,"Rock"]))
```

```{r}
neigh = neigh.create(type = 0)
res = kriging(jurarg,gridrg,m,neigh)
plot(res,sub="Cokriging of Co")
```

Now perform Ordinary Cokriging on the validation set.

```{r}
val_locrg = db.create(val_loc)
val_locrg = db.locate(val_locrg,c("Xloc","Yloc"),"x")
res_val = kriging(jurarg,val_locrg,m,neigh)
mean((res_val[,"Kriging.Co.estim"]-val[,2])^2)  ## MSE for Cokriging
```

We could also attempt to plot all the vmaps and the standard deviation maps as follows : 

```{r}
neigh = neigh.create(type = 0)
res = kriging(jurarg,gridrg,m,neigh)
plot(res,sub="Cokriging of Co")
v=vmap.calc(jurarg)
std_co=sqrt(v$items[["VMAP.Co"]])
std_zn_co=sqrt(v$items[["VMAP.Zn.Co"]])
std_zn=sqrt(v$items[["VMAP.Zn"]])
std_cu_co=sqrt(v$items[["VMAP.Cu.Co"]])
std_cu_zn=sqrt(v$items[["VMAP.Cu.Zn"]])
std_cu=sqrt(v$items[["VMAP.Cu"]])
v1=db.add(v,std_co,std_zn_co,std_zn,std_cu_co,std_cu_zn,std_cu)
par(mfrow=c(2,3))
plot(v1,pos.x = 1,pos.y = 2,name="VMAP.Co")
plot(v1,pos.x = 1,pos.y = 2,name="VMAP.Zn.Co")
plot(v1,pos.x = 1,pos.y = 2,name="VMAP.Zn")
plot(v1,pos.x = 1,pos.y = 2,name="VMAP.Cu.Co")
plot(v1,pos.x = 1,pos.y = 2,name="VMAP.Cu.Zn")
plot(v1,pos.x = 1,pos.y = 2,name="VMAP.Cu")
plot(v1,pos.x = 1,pos.y = 2,name="std_co")
plot(v1,pos.x = 1,pos.y = 2,name="std_zn_co")
plot(v1,pos.x = 1,pos.y = 2,name="std_zn")
plot(v1,pos.x = 1,pos.y = 2,name="std_cu_co")
plot(v1,pos.x = 1,pos.y = 2,name="std_cu_zn")
plot(v1,pos.x = 1,pos.y = 2,name="std_cu")

```

Now we can compare this with the ordinary kriging results by just removing Zn and Cu from our regionalized variables and computing the prediction score once again :

```{r}
jurarg <- db.locate(jurarg, c("Zn","Cu"))# Remove Zn,Cu

# Variogram
v = vario.calc(jurarg,nlag=12)
m = model.auto(v,c(1,3,3))

# Ordinary Kriging
neigh = neigh.create(type = 0)
res=kriging(jurarg,gridrg,m,neigh) 
plot(res,sub="Kriging of Co")

# Ordinary Kriging on val
res_val=kriging(jurarg,val_locrg,m,neigh)
mean((res_val[,"Kriging*estim"]-val[,2])^2)  ## MSE for Ordinary kriging
```

We get the following prediction score : 6.939636 which is better than the case where Zn and Cu were a part of the regionalized variables.
We could also compare this to the universal kriging case where we got a score of 7.088469.

# Maximum Likelihood estimation

## Maximum Likelihood estimation
Here we compute the maximum likelihood estimates of the following model for the cobalt concentrations:
$$Co(x) = \mu + Y(x)$$
where $Co$ represents the cobalt concentration, $\mu$ a constant mean and $Y$ a centered Gaussian random field. 

We will also consider models of the form:
$$Co(x) = \mu(x) + Y(x)$$
where $\mu(x)$ may vary according to the location, e.g. $\mu(x) = X(x)\beta$ with $X$ an explanatory variable.


To do this, we use the **geoR** package. You may need to install it :

```{r, eval = FALSE}
indiccut.rock = limits.create(mini=c(1,2,3),maxi=c(2,3,4))
indiccut.lu = limits.create(mini=c(1,2),maxi=c(2,3))


jurarg_KU = db.indicator(jurarg,indiccut.rock,name="Rock")
jurarg_KU = db.indicator(jurarg_KU,indiccut.lu,name="Landuse")

jurarg_KU = db.locate(jurarg_KU,"Indicator*","f")
jurarg_KU = db.locate(jurarg_KU,"Co","z")

gridrg_KU=db.indicator(gridrg,indiccut.rock,name="Rock")
gridrg_KU=db.indicator(gridrg,indiccut.rock,name="Rock")
gridrg_KU = db.locate(gridrg_KU,"Indicator*","f")
gridrg_KU = db.locate(gridrg_KU,"Co","z")

val_locrg_KU=db.indicator(val_locrg,indiccut.rock,name="Rock")
val_locrg_KU = db.locate(val_locrg_KU,"Indicator*","f")
val_locrg_KU = db.locate(val_locrg_KU,"Co","z")
install.packages('geoR')
```


**geoR** uses a particular class to store spatial data sets : **geodata** (similar to a Rgeostats db). The first thing is hence to transform our data in that class.

```{r}
library(geoR)
jura.geoR = as.geodata(jurarg_KU[], coords.col = 2:3, data.col = 7,covar.col = 13:15)
plot(jura.geoR)
```

We can also compute variograms in **geoR**.


Without Landuse:

Omnidirectional
```{r}
par(mfrow=c(1,2))
vg <- variog(jura.geoR,uvec = seq(0,4,l=20), bin.cloud = T)
plot(vg)
```
Check for anisotropies
```{r}
vg.60 <- variog(jura.geoR,uvec = seq(0,4,l=20), bin.cloud = T,direction = pi/3)
plot(vg.60)
```


Then according to the covariance structure we want to infer (exponential by default), we can try something like this (see **?likfit** for more details)

```{r}
mean(jura$Co)
var(jura$Co)
ml =likfit(jura.geoR, ini = c(10,0.5), nug = 0.5,fix.psiA = F, psiA = pi/3,fix.psiR = F, psiR=1, trend = trend.spatial(trend = ~Indicator.Rock.1 + Indicator.Rock.2 + Indicator.Rock.3,jura.geoR))
summary(ml)
ml2 = likfit(jura.geoR, ini = c(10,0.5), nug = 0.5,fix.psiA = F, psiA = pi/3,fix.psiR = F, psiR=1, trend = trend.spatial(trend = ~Indicator.Rock.1 + Indicator.Rock.2,jura.geoR))
ml3 = likfit(jura.geoR, ini = c(10,0.5), nug = 0.5,fix.psiA = F, psiA = pi/3,fix.psiR = F, psiR=1)
```

Likelihood ratio test
```{r}
T1.2 = 2 *(ml$loglik-ml2$loglik)
T3.1 = 2 *(ml$loglik-ml3$loglik)
T3.2 = 2 *(ml2$loglik-ml3$loglik)
1-pchisq(T1.2,1) 
1-pchisq(T3.2,2) 
1-pchisq(T3.1,3)
```


We can represent the resulting fitting on the variogram

```{r}
plot(vg)
lines(ml)

plot(vg.60)
lines(ml)
par(mfrow=c(1,1))
```


## Prediction

We perform kriging with the **geoR** functions, on the grid and on the validation locations.

```{r}
k.grid = krige.conv(jura.geoR,loc =grid[,1:2],krige = krige.control(obj.m=ml3))
k.val_loc = krige.conv(jura.geoR,loc = val_loc[,1:2],krige = krige.control(obj.m=ml))
```

Next we plot the resulting kriging and standard deviation maps using RGeostats and compute the MSE on the validation locations.

```{r}
gridrg = add.variable(k.grid$predict,gridrg,"geoR.k")
plot(gridrg)
gridrg = add.variable(sqrt(k.grid$krige.var),gridrg,"geoR.ks")
plot(gridrg)

MSE_ML = mean((k.val_loc$predict-val[,2])^2)
print(MSE_ML)
```
MSE without Landuuse: 6.988




1. Compute the maximum likelihood estimator of the parameters of (some of) your favorite univariate model(s) for the Cobalt concentration. 

With Landuse:

Omnidirectional
```{r}
jura.geoRL = as.geodata(jurarg_KU[], coords.col = 2:3, data.col = 7,covar.col = 13:17)
plot(jura.geoRL)
par(mfrow=c(1,2))
vg <- variog(jura.geoRL,uvec = seq(0,4,l=20), bin.cloud = T)
plot(vg)
```
Check for anisotropies
```{r}
vg.60 <- variog(jura.geoRL,uvec = seq(0,4,l=20), bin.cloud = T,direction = pi/3)
plot(vg.60)
```


Then according to the covariance structure we want to infer (exponential by default), we can try something like this (see **?likfit** for more details)

```{r}
mean(jura$Co)
var(jura$Co)
ml =likfit(jura.geoRL, ini = c(10,0.5), nug = 0.5,fix.psiA = F, psiA = pi/3,fix.psiR = F, psiR=1, trend = trend.spatial(trend = ~Indicator.Rock.1 + Indicator.Rock.2 + Indicator.Rock.3 + Indicator.Landuse.1 + Indicator.Landuse.2,jura.geoRL))
summary(ml)
ml2 = likfit(jura.geoRL, ini = c(10,0.5), nug = 0.5,fix.psiA = F, psiA = pi/3,fix.psiR = F, psiR=1, trend = trend.spatial(trend = ~Indicator.Rock.1 + Indicator.Rock.2+ Indicator.Landuse.1,jura.geoRL))
ml3 = likfit(jura.geoRL, ini = c(10,0.5), nug = 0.5,fix.psiA = F, psiA = pi/3,fix.psiR = F, psiR=1)
```

Likelihood ratio test
```{r}
T1.2 = 2 *(ml$loglik-ml2$loglik)
T3.1 = 2 *(ml$loglik-ml3$loglik)
T3.2 = 2 *(ml2$loglik-ml3$loglik)
1-pchisq(T1.2,1) 
1-pchisq(T3.2,2) 
1-pchisq(T3.1,3) 
```


We can represent the resulting fitting on the variogram

```{r}
plot(vg)
lines(ml)

plot(vg.60)
lines(ml)
par(mfrow=c(1,1))
```


## Prediction

We perform kriging with the **geoR** functions, on the grid and on the validation locations.

```{r}
k.grid = krige.conv(jura.geoRL,loc =grid[,1:2],krige = krige.control(obj.m=ml3))
k.val_loc = krige.conv(jura.geoRL,loc = val_loc[,1:2],krige = krige.control(obj.m=ml))
```

Next we plot the resulting kriging and standard deviation maps using RGeostats and compute the MSE on the validation locations.

```{r}
gridrg = add.variable(k.grid$predict,gridrg,"jura.geoRL.k")
plot(gridrg)
gridrg = add.variable(sqrt(k.grid$krige.var),gridrg,"jura.geoRL.ks")
plot(gridrg)

val_loc_k = read.csv("jura/jura_val_loc.csv")
MSE_ML = mean((k.val_loc$predict-val[,2])^2)
print(MSE_ML)
```
MSE : 6.977



## Bayesian approach
Same model now but we adopt the Bayesian point of view to better quantify the uncertainties. First, we need to characterize the prior. Note that there is no error analysis available for the anisotropy parameters. Here we use the default for most parameters (see **?prior.control**). We discretize the prior for the range parameter to limit the computations.


```{r}
pr <- prior.control(phi.discrete = seq(0, 1, l=101), phi.prior="rec")
```
Now we predict on the validation locations.


```{r}
bsp <- krige.bayes(jura.geoR,loc = val_loc[,1:2],prior = pr, output = output.control(n.post=10000))
```

Lets have a look at the results. First the posterior distributions of the parameters.


```{r}
hist(bsp$posterior$sample$beta, main="", xlab=expression(beta), prob = T)
hist(bsp$posterior$sample$sigmasq, main="", xlab=expression(sigma^2), prob = T)
hist(bsp$posterior$sample$phi, main="", xlab=expression(phi), prob = T)
```

Now in terms of variogram

```{r}
plot(vg,ylim=c(0,20))
lines(bsp, max.dist = 4, summ = mean)
lines(bsp, max.dist = 4, summ = median, lty = 2)
lines(bsp, max.dist = 4, summ = "mode", post="par",lwd = 2, lty = 2)
legend(0, 21, legend = c("variogram posterior mean", "variogram posterior median", "parameters posterior mode"), lty = c(1,2,2), lwd = c(1,1,2), cex = 0.8)
```

We now look at the posterior prediction at 4 uniformly sampled points among the validation locations.


```{r}
par(mfrow=c(2,2))
locations.bayes = sample(1:59,4)
for(i in locations.bayes){
  ## curve(dnorm(x, mean=k.val_loc$pred[i], sd=sqrt(k.val_loc$krige.var[i])), from=k.val_loc$pred[i] - 3*sqrt(k.val_loc$krige.var[i]), k.val_loc$pred[i] +3*sqrt(k.val_loc$krige.var[i]))
  kpx <- seq(k.val_loc$pred[i] - 3*sqrt(k.val_loc$krige.var[i]), k.val_loc$pred[i] +3*sqrt(k.val_loc$krige.var[i]), l=100)
  kpy <- dnorm(kpx, mean=k.val_loc$pred[i], sd=sqrt(k.val_loc$krige.var[i]))
  bp <- density(bsp$predictive$simulations[i,])
  rx <- range(c(kpx, bp$x))
  ry <- range(c(kpy, bp$y))
  plot(cbind(rx, ry), type="n", xlab=paste("Location", i), ylab="density", xlim=range(kpx), ylim=range(kpy))
  lines(kpx, kpy, lty=2)
  lines(bp)
  }
```

We compute the score taking the posterior mean as the predictor


```{r}
MSE_bayes.mean = mean((bsp$predictive$mean-val[,2])^2)
print(MSE_bayes.mean)
```

We compute the score taking the posterior median as the predictor


```{r}
MSE_bayes.med = mean((apply(bsp$predictive$simulations,1,quantile,probs = .5)-val[,2])^2)
print(MSE_bayes.med)
```


Now on the grid. This may take some time...


```{r}
bsp4 <- krige.bayes(jura.geoR,loc = grid[,1:2],prior = pr, output = output.control(n.predictive=2))
```
Lets have a look at the results
```{r}


gridrg = add.variable(bsp4$predictive$mean,gridrg,"geoR.bk")
plot(gridrg,title='Bayesian kriging map')
gridrg = add.variable(sqrt(bsp4$predictive$variance),gridrg,"geoR.bks")
plot(gridrg,title='Bayesian standard deviation map')
# simulations 
gridrg = add.variable(bsp4$predictive$simulations[,1],gridrg,"geoR.bs1")
plot(gridrg,title='simulation 1')
gridrg = add.variable(bsp4$predictive$simulations[,2],gridrg,"geoR.bs2")
plot(gridrg,title='simulation 2')
```


# Conditional simulations

The information threshold for the concentration of cobalt in soils is *12 mg/kg*. 

1. Generate 100 conditional simulations of the Cobalt concentrations over the swiss Jura according to your favorite model. 

Example with the ordinary kriging (where mres is the fitted variogram of the cobalt)

```{r}
res_simu = simtub(jurarg_KU,gridrg_KU,mres,neigh,nbsimu=100,nbtuba=1000)
```

2. Compute the mean surface of the area of exceedance as well as its associated centered 95% confidence interval.
```{r}
res_simu[,"Simu.Co.S*"] = res_simu[,"Simu.Co.S*"]>12
res_simu[,"Simu.Co.S1"] <- rowMeans(res_simu[,"Simu.Co.S*"])
print("The mean surface of the area of exceedance is:" + mean(rowMeans(res_simu[,"Simu.Co.S*"])))

```

3. Compute and plot the exceedance probability map. Comment.
```{r}
plot(res_simu,name="Simu.Co.S1",pos.legend=1)
```

# Summary -- Discussion



